# Pitfalls Research: Intelligent Automation & Organization (v1.4)

**Project:** SAAP 2026 v2
**Context:** Adding task hierarchies, supplier management, activity logging, bidirectional sync, and AI semantic matching to existing Next.js 14/Prisma/MariaDB application
**Researched:** 2026-01-24
**Overall Confidence:** HIGH (verified with official Prisma docs, established database patterns, and industry research)

---

## Hierarchical Data Pitfalls (Tasks/Subtasks)

Common mistakes when implementing infinite nesting hierarchies in Prisma with MariaDB.

### Pitfall 1: Unbounded Recursive Includes in Prisma

**What goes wrong:** Attempting to fetch entire task tree with nested `include` statements leads to N+1 query problems, memory exhaustion, and timeout errors.

**Why it happens:** Prisma does not support recursive includes natively. Developers try to work around this by manually nesting include statements to arbitrary depth, but this creates exponentially more queries.

**Consequences:**
- Performance degradation as task trees grow
- Memory exhaustion on deep hierarchies (10+ levels)
- N+1 query explosion (each level = additional query per item)
- Timeout errors on large task trees
- Client-side crashes trying to render massive JSON

**Warning signs:**
- Slow task loading as subtask count grows
- Database connection pool exhaustion
- "Maximum call stack exceeded" errors
- Query timeouts in Prisma logs

**Prevention:**
```typescript
// BAD: Attempting to manually nest includes
const task = await prisma.task.findUnique({
  where: { id },
  include: {
    children: {
      include: {
        children: {
          include: {
            children: true // How deep? Unknown!
          }
        }
      }
    }
  }
});

// GOOD: Fetch flat list, build tree client-side
async function getTaskTree(rootId: string) {
  // Get all descendants in one query using raw SQL CTE
  const descendants = await prisma.$queryRaw<Task[]>`
    WITH RECURSIVE task_tree AS (
      SELECT * FROM tasks WHERE id = ${rootId}
      UNION ALL
      SELECT t.* FROM tasks t
      INNER JOIN task_tree tt ON t.parent_id = tt.id
    )
    SELECT * FROM task_tree
    LIMIT 1000; -- Safety limit
  `;

  // Build tree structure in-memory
  return buildTree(descendants, rootId);
}

function buildTree(tasks: Task[], rootId: string): TaskNode {
  const taskMap = new Map(tasks.map(t => [t.id, { ...t, children: [] }]));
  const root = taskMap.get(rootId);

  for (const task of tasks) {
    if (task.parentId && task.parentId !== rootId) {
      const parent = taskMap.get(task.parentId);
      if (parent) parent.children.push(taskMap.get(task.id)!);
    }
  }

  return root;
}
```

**Phase to address:** Phase 1 (Schema Design) - Use adjacency list with CTE queries from start

**Sources:**
- [Prisma Self-Relations Documentation](https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/self-relations)
- [Prisma Issue #3725 - Recursive Relationships](https://github.com/prisma/prisma/issues/3725)
- [NestJS Recursive Relationships with Prisma](https://wanago.io/2023/12/11/api-nestjs-sql-recursive-relationships-prisma-postgresql/)

---

### Pitfall 2: Cascade Delete Without Depth Control

**What goes wrong:** Deleting a parent task triggers cascading deletes through entire subtask tree, potentially removing thousands of records in a single operation.

**Why it happens:** Using `onDelete: Cascade` on self-referential relations without considering the scope of deletion.

**Consequences:**
- Accidental mass data deletion
- Long-running delete transactions that lock tables
- Timeout errors on large tree deletions
- No undo capability for cascaded deletes

**Warning signs:**
- "Delete task" operation taking unusually long
- Lock wait timeouts on delete operations
- More rows deleted than expected
- Users reporting "all my subtasks disappeared"

**Prevention:**
```prisma
// Prisma schema with safer deletion approach
model Task {
  id        String  @id @default(cuid())
  title     String
  parentId  String? @map("parent_id")
  parent    Task?   @relation("TaskToSubtask", fields: [parentId], references: [id], onDelete: SetNull)
  children  Task[]  @relation("TaskToSubtask")
  // ...
}
```

```typescript
// Explicit recursive delete with confirmation
async function deleteTaskWithSubtasks(taskId: string) {
  // First, count affected tasks
  const descendants = await prisma.$queryRaw<{ count: number }[]>`
    WITH RECURSIVE task_tree AS (
      SELECT id FROM tasks WHERE id = ${taskId}
      UNION ALL
      SELECT t.id FROM tasks t
      INNER JOIN task_tree tt ON t.parent_id = tt.id
    )
    SELECT COUNT(*) as count FROM task_tree
  `;

  const count = descendants[0].count;

  // Require explicit confirmation for large deletions
  if (count > 10) {
    throw new Error(`This will delete ${count} tasks. Use deleteTaskWithSubtasksConfirmed.`);
  }

  // Delete in reverse order (leaves first)
  await prisma.$executeRaw`
    WITH RECURSIVE task_tree AS (
      SELECT id, 0 as depth FROM tasks WHERE id = ${taskId}
      UNION ALL
      SELECT t.id, tt.depth + 1 FROM tasks t
      INNER JOIN task_tree tt ON t.parent_id = tt.id
    )
    DELETE FROM tasks WHERE id IN (SELECT id FROM task_tree ORDER BY depth DESC)
  `;
}
```

**Phase to address:** Phase 1 (Schema Design) - Configure onDelete: SetNull, implement explicit tree deletion

---

### Pitfall 3: UI Renders Entire Tree Without Virtualization

**What goes wrong:** Attempting to render 1000+ nested task nodes causes browser freeze, massive DOM size, and poor user experience.

**Why it happens:** Rendering nested tree structures as fully expanded DOM elements without virtualization or lazy loading.

**Consequences:**
- Browser tab crashes or freezes
- Slow initial page load
- Poor scroll performance
- Memory leaks from unmounted nodes

**Warning signs:**
- Task list page taking 5+ seconds to render
- Browser developer tools showing thousands of DOM nodes
- Scroll lag on task views
- Users avoiding task pages with many subtasks

**Prevention:**
```typescript
// Use react-window or virtualized tree components
import { VariableSizeTree as Tree } from 'react-vtree';

// Only render visible nodes + buffer
function VirtualizedTaskTree({ tasks }: { tasks: TaskNode[] }) {
  const getNodeData = (node: TaskNode, nestingLevel: number) => ({
    data: {
      id: node.id,
      isLeaf: node.children.length === 0,
      isOpenByDefault: nestingLevel < 2, // Only expand first 2 levels
      name: node.title,
      nestingLevel,
    },
    nestingLevel,
    node,
  });

  return (
    <Tree
      treeWalker={treeWalker}
      itemSize={50}
      height={600}
      width="100%"
    >
      {TaskRow}
    </Tree>
  );
}

// Alternative: Lazy load children on expand
function LazyTaskNode({ task }: { task: Task }) {
  const [children, setChildren] = useState<Task[] | null>(null);
  const [isExpanded, setIsExpanded] = useState(false);

  const handleExpand = async () => {
    if (!children) {
      const fetched = await fetch(`/api/tasks/${task.id}/children`).then(r => r.json());
      setChildren(fetched);
    }
    setIsExpanded(!isExpanded);
  };

  return (
    <div>
      <button onClick={handleExpand}>{isExpanded ? '-' : '+'}</button>
      <span>{task.title}</span>
      {isExpanded && children?.map(child => (
        <LazyTaskNode key={child.id} task={child} />
      ))}
    </div>
  );
}
```

**Phase to address:** Phase 3 (Task UI) - Implement virtualized or lazy-loaded tree from start

---

### Pitfall 4: No Maximum Depth Enforcement

**What goes wrong:** Users create deeply nested tasks (20+ levels), causing performance issues and confusing UX.

**Why it happens:** No business logic to limit nesting depth. "Infinite nesting" taken too literally.

**Consequences:**
- Extreme indentation makes tasks unreadable
- Performance degrades with depth
- Mobile users cannot navigate deep trees
- Export/print becomes impossible

**Warning signs:**
- Tasks indented off-screen
- Users complaining about "lost" subtasks
- Horizontal scroll required to see task titles
- Recursive queries hitting depth limits

**Prevention:**
```typescript
const MAX_TASK_DEPTH = 5; // Reasonable limit for SAAP

async function createTask(data: CreateTaskInput) {
  if (data.parentId) {
    // Calculate parent depth
    const depth = await getTaskDepth(data.parentId);

    if (depth >= MAX_TASK_DEPTH) {
      throw new Error(`Maximum nesting depth (${MAX_TASK_DEPTH}) reached. Consider reorganizing tasks.`);
    }
  }

  return prisma.task.create({ data });
}

async function getTaskDepth(taskId: string): Promise<number> {
  const result = await prisma.$queryRaw<{ depth: number }[]>`
    WITH RECURSIVE ancestors AS (
      SELECT id, parent_id, 0 as depth FROM tasks WHERE id = ${taskId}
      UNION ALL
      SELECT t.id, t.parent_id, a.depth + 1
      FROM tasks t
      INNER JOIN ancestors a ON t.id = a.parent_id
    )
    SELECT MAX(depth) as depth FROM ancestors
  `;

  return result[0].depth;
}
```

**Phase to address:** Phase 2 (Task CRUD) - Enforce depth limit in create/move operations

**Sources:**
- [Asana Task Nesting Depth Discussion](https://forum.asana.com/t/limit-sub-task-depth/8510)
- [Managing Infinite Nesting with Notion](https://ones.com/blog/manage-infinite-nesting-notion-nested-pages/)

---

### Pitfall 5: Moving Tasks Creates Cycles

**What goes wrong:** When reparenting tasks, a task is moved to become a child of its own descendant, creating a cycle in the tree.

**Why it happens:** Validation only checks immediate parent, not entire ancestry chain.

**Consequences:**
- Infinite loops in tree traversal
- Database corruption (conceptually)
- Recursive queries never terminate
- Application crashes

**Warning signs:**
- "Maximum recursion depth exceeded" errors
- CPU spikes during task tree operations
- Tasks "disappearing" from views
- Inconsistent parent/child relationships

**Prevention:**
```typescript
async function moveTask(taskId: string, newParentId: string) {
  // Cannot move to itself
  if (taskId === newParentId) {
    throw new Error('Cannot move task to itself');
  }

  // Cannot move to a descendant
  const isDescendant = await checkIsDescendant(newParentId, taskId);
  if (isDescendant) {
    throw new Error('Cannot move task to its own descendant');
  }

  return prisma.task.update({
    where: { id: taskId },
    data: { parentId: newParentId }
  });
}

async function checkIsDescendant(potentialDescendant: string, ancestorId: string): Promise<boolean> {
  const result = await prisma.$queryRaw<{ found: number }[]>`
    WITH RECURSIVE ancestors AS (
      SELECT id, parent_id FROM tasks WHERE id = ${potentialDescendant}
      UNION ALL
      SELECT t.id, t.parent_id
      FROM tasks t
      INNER JOIN ancestors a ON t.id = a.parent_id
    )
    SELECT COUNT(*) as found FROM ancestors WHERE id = ${ancestorId}
  `;

  return result[0].found > 0;
}
```

**Phase to address:** Phase 2 (Task Operations) - Validate ancestry before reparenting

---

## AI Semantic Matching Pitfalls

Common mistakes when using AI embeddings for line item matching and price comparison.

### Pitfall 6: Using Embeddings for Numerical Comparison

**What goes wrong:** Attempting to use semantic embeddings to compare prices or quantities. Embeddings capture meaning, not mathematical relationships.

**Why it happens:** Misunderstanding what embeddings do. Treating "product under $300" as a semantic query when it requires numerical filtering.

**Consequences:**
- Incorrect price comparisons
- "Similar" products with wildly different prices
- Missed matches due to price difference confusion
- Users lose trust in comparison features

**Warning signs:**
- Search for "cheapest supplier" returns wrong results
- Price filters don't work as expected
- "Most affordable" shows expensive items
- Numerical queries returning semantically similar but numerically wrong results

**Prevention:**
```typescript
// Hybrid approach: Semantic matching + structured filtering
async function compareSupplierPrices(lineItemDescription: string) {
  // 1. Get embeddings for semantic matching of product descriptions
  const embedding = await getEmbedding(lineItemDescription);

  // 2. Find semantically similar items across suppliers
  const similarItems = await vectorSearch(embedding, {
    threshold: 0.85, // High threshold for line item matching
    limit: 50
  });

  // 3. Filter and sort by NUMERICAL price (not embedding similarity)
  const withPrices = await prisma.supplierItem.findMany({
    where: {
      id: { in: similarItems.map(i => i.id) }
    },
    include: {
      supplier: true,
      priceHistory: {
        orderBy: { effectiveDate: 'desc' },
        take: 1
      }
    }
  });

  // 4. Group by semantic cluster, sort by actual price
  return groupBySimilarity(withPrices)
    .map(group => ({
      ...group,
      items: group.items.sort((a, b) =>
        a.priceHistory[0].price - b.priceHistory[0].price
      )
    }));
}
```

**Phase to address:** Phase 4 (AI Matching) - Use hybrid semantic + numerical approach

**Sources:**
- [OpenAI Community - Embeddings Price Problem](https://community.openai.com/t/ecommerce-product-catalog-search-embeddings-problem-with-price/1036804)
- [Semantic Similarity Pitfalls](https://agent-ci.com/blog/2025/10/08/semantic-similarity-nuanced-not-difficult/)

---

### Pitfall 7: Treating Similarity Score as Single Truth

**What goes wrong:** Using a single similarity threshold for all matching decisions. "0.8 means match" applied universally.

**Why it happens:** Simplicity. One threshold is easier than nuanced matching rules.

**Consequences:**
- False positives (different products matched)
- False negatives (same product not matched)
- Inconsistent matching across product categories
- Users manually correcting matches constantly

**Warning signs:**
- "Printer Paper A4" matching "Printer Ink Cartridge"
- Identical products from different suppliers not matching
- Similarity threshold constantly being adjusted
- Users complaining about wrong matches

**Prevention:**
```typescript
interface MatchResult {
  score: number;
  confidence: 'HIGH' | 'MEDIUM' | 'LOW';
  requiresReview: boolean;
}

function evaluateMatch(similarity: number, context: MatchContext): MatchResult {
  // Different thresholds for different contexts
  const thresholds = {
    exactProduct: { high: 0.95, medium: 0.90, low: 0.85 },
    categoryMatch: { high: 0.80, medium: 0.70, low: 0.60 },
    brandVariant: { high: 0.88, medium: 0.82, low: 0.75 }
  };

  const threshold = thresholds[context.matchType];

  if (similarity >= threshold.high) {
    return { score: similarity, confidence: 'HIGH', requiresReview: false };
  } else if (similarity >= threshold.medium) {
    return { score: similarity, confidence: 'MEDIUM', requiresReview: true };
  } else if (similarity >= threshold.low) {
    return { score: similarity, confidence: 'LOW', requiresReview: true };
  }

  return { score: similarity, confidence: 'LOW', requiresReview: true };
}

// Show confidence to user
function MatchIndicator({ match }: { match: MatchResult }) {
  return (
    <Badge variant={match.confidence === 'HIGH' ? 'success' : 'warning'}>
      {match.confidence} ({Math.round(match.score * 100)}%)
      {match.requiresReview && <span className="ml-1">Review suggested</span>}
    </Badge>
  );
}
```

**Phase to address:** Phase 4 (AI Matching) - Implement confidence levels with human review

**Sources:**
- [Semantic Similarity Comprehensive Guide 2025](https://www.shadecoder.com/topics/semantic-similarity-a-comprehensive-guide-for-2025)
- [Galileo STS Metric Guide](https://galileo.ai/blog/semantic-textual-similarity-metric)

---

### Pitfall 8: No Fallback for AI Service Failures

**What goes wrong:** AI embedding service (Voyage AI, OpenAI) goes down, and entire price comparison feature becomes unavailable.

**Why it happens:** Tight coupling to external AI service without graceful degradation.

**Consequences:**
- Feature completely unavailable during outages
- Users cannot compare prices when they need to
- No cached results to fall back on
- Business process blocked by AI dependency

**Warning signs:**
- "Cannot compare prices" errors during API outages
- Entire supplier comparison page fails to load
- No cached embeddings for previously matched items
- External service errors bubble up to UI

**Prevention:**
```typescript
// Cache embeddings for resilience
model SupplierItem {
  id          String   @id @default(cuid())
  description String
  embedding   Bytes?   // Cached embedding vector
  embeddingAt DateTime? @map("embedding_at")
  // ...
}

async function getItemEmbedding(item: SupplierItem): Promise<number[]> {
  // Use cached embedding if fresh (within 30 days)
  if (item.embedding && item.embeddingAt) {
    const age = Date.now() - item.embeddingAt.getTime();
    if (age < 30 * 24 * 60 * 60 * 1000) {
      return deserializeEmbedding(item.embedding);
    }
  }

  try {
    // Try to get fresh embedding
    const embedding = await voyageAI.embed(item.description);

    // Cache for future use
    await prisma.supplierItem.update({
      where: { id: item.id },
      data: {
        embedding: serializeEmbedding(embedding),
        embeddingAt: new Date()
      }
    });

    return embedding;
  } catch (error) {
    // Fallback to cached embedding even if stale
    if (item.embedding) {
      console.warn('Using stale embedding due to API error');
      return deserializeEmbedding(item.embedding);
    }

    // Final fallback: keyword-based matching
    console.warn('Falling back to keyword matching');
    return null; // Signal to use BM25/keyword matching
  }
}

// Hybrid matching with fallback
async function findSimilarItems(description: string): Promise<SupplierItem[]> {
  const embedding = await getEmbedding(description).catch(() => null);

  if (embedding) {
    return vectorSearch(embedding);
  } else {
    // Fallback to full-text search
    return prisma.supplierItem.findMany({
      where: {
        description: { contains: description, mode: 'insensitive' }
      }
    });
  }
}
```

**Phase to address:** Phase 4 (AI Infrastructure) - Cache embeddings and implement fallbacks

---

### Pitfall 9: Embedding Cost Explosion

**What goes wrong:** Every price comparison triggers new embedding API calls, leading to unexpectedly high AI costs.

**Why it happens:** Not caching embeddings, re-embedding same text repeatedly.

**Consequences:**
- API bills higher than expected
- Rate limiting from embedding provider
- Slow comparisons due to API latency
- Feature becomes too expensive to use

**Warning signs:**
- Embedding API costs growing faster than usage
- Rate limit errors during bulk operations
- Same descriptions being embedded multiple times
- Monthly API bill surprises

**Prevention:**
```typescript
// Embed on import, not on query
async function importSupplierQuote(quote: ParsedQuote) {
  // Batch embed all new line items
  const newItems = quote.lineItems.filter(item =>
    !existingItems.some(e => e.description === item.description)
  );

  if (newItems.length > 0) {
    // Batch API call (cheaper than individual calls)
    const embeddings = await voyageAI.embedBatch(
      newItems.map(i => i.description)
    );

    // Store embeddings with items
    await prisma.$transaction(
      newItems.map((item, i) =>
        prisma.supplierItem.create({
          data: {
            ...item,
            embedding: serializeEmbedding(embeddings[i]),
            embeddingAt: new Date()
          }
        })
      )
    );
  }
}

// Monitor and budget
const MONTHLY_EMBEDDING_BUDGET = 1_000_000; // tokens

async function checkEmbeddingBudget(): Promise<boolean> {
  const usage = await prisma.aiUsage.aggregate({
    where: {
      type: 'embedding',
      createdAt: {
        gte: startOfMonth(new Date())
      }
    },
    _sum: { tokens: true }
  });

  return (usage._sum.tokens || 0) < MONTHLY_EMBEDDING_BUDGET;
}
```

**Phase to address:** Phase 4 (AI Infrastructure) - Batch embeddings on import, cache results

**Sources:**
- [OpenAI Embeddings Pricing](https://costgoat.com/pricing/openai-embeddings)
- [Embedding Models in 2025](https://medium.com/@alex-azimbaev/embedding-models-in-2025-technology-pricing-practical-advice-2ed273fead7f)

---

## Activity/Audit Logging Pitfalls

Common mistakes when implementing activity history for entities.

### Pitfall 10: Logging Everything to Single Table

**What goes wrong:** All activity logs go to one table that grows unboundedly, causing query performance degradation and storage bloat.

**Why it happens:** Simplest design - one ActivityLog table for all entities. Works at first, becomes problem at scale.

**Consequences:**
- Activity table becomes largest table in database
- Queries for specific entity history slow down
- Backup and restore times increase
- Database maintenance becomes difficult

**Warning signs:**
- Activity log table size exceeds all other tables combined
- "Show history" feature getting slower
- Database backups taking longer each week
- Index maintenance consuming disk space

**Prevention:**
```prisma
// Option 1: Separate tables per entity type (easier querying)
model DealActivity {
  id        String   @id @default(cuid())
  dealId    String   @map("deal_id")
  deal      Deal     @relation(fields: [dealId], references: [id], onDelete: Cascade)
  action    String   // 'created', 'updated', 'stage_changed'
  changes   Json     // { field: { old, new } }
  userId    String   @map("user_id")
  createdAt DateTime @default(now())

  @@index([dealId, createdAt])
  @@map("deal_activities")
}

model ProjectActivity {
  id        String   @id @default(cuid())
  projectId String   @map("project_id")
  project   Project  @relation(fields: [projectId], references: [id], onDelete: Cascade)
  action    String
  changes   Json
  userId    String   @map("user_id")
  createdAt DateTime @default(now())

  @@index([projectId, createdAt])
  @@map("project_activities")
}

// Option 2: Single table with polymorphic reference (simpler schema)
model Activity {
  id          String   @id @default(cuid())
  entityType  String   // 'deal', 'project', 'potential'
  entityId    String   @map("entity_id")
  action      String
  changes     Json
  userId      String   @map("user_id")
  createdAt   DateTime @default(now())

  @@index([entityType, entityId, createdAt])
  @@map("activities")
}
```

**Phase to address:** Phase 2 (Activity Schema) - Choose appropriate table strategy based on query patterns

**Sources:**
- [Database Design for Audit Logging](https://vertabelo.com/blog/database-design-for-audit-logging/)
- [Martin Fowler - Audit Log Pattern](https://martinfowler.com/eaaDev/AuditLog.html)

---

### Pitfall 11: No Log Retention Policy

**What goes wrong:** Activity logs accumulate forever, consuming storage and degrading performance.

**Why it happens:** "We might need the history" without defining retention requirements.

**Consequences:**
- Activity table grows 10-100x faster than business data
- Older logs rarely accessed but still queried
- Storage costs increase linearly with time
- Cannot efficiently query recent history

**Warning signs:**
- Activity table has millions of rows
- Most logs older than 1 year
- Storage alerts on database
- "Show recent activity" includes years-old entries

**Prevention:**
```typescript
// Define retention tiers
const RETENTION_POLICY = {
  detail: 90,    // Keep full change details for 90 days
  summary: 365,  // Keep action summaries for 1 year
  archive: null  // Keep counts/aggregates forever
};

// Scheduled cleanup job (run weekly)
async function cleanupOldActivities() {
  const detailCutoff = subDays(new Date(), RETENTION_POLICY.detail);
  const summaryCutoff = subDays(new Date(), RETENTION_POLICY.summary);

  // Remove detail from old entries (keep summary)
  await prisma.activity.updateMany({
    where: {
      createdAt: { lt: detailCutoff },
      changes: { not: null }
    },
    data: {
      changes: null // Remove detailed change data
    }
  });

  // Delete very old entries entirely
  const deleted = await prisma.activity.deleteMany({
    where: {
      createdAt: { lt: summaryCutoff }
    }
  });

  console.log(`Cleaned up ${deleted.count} old activity entries`);
}

// Add to schema
model Activity {
  // ... fields

  // Partition hint for future optimization
  @@index([createdAt]) // For cleanup queries
}
```

**Phase to address:** Phase 2 (Activity Infrastructure) - Define and implement retention policy

---

### Pitfall 12: Logging in Transaction Creates Contention

**What goes wrong:** Activity logging inside business transaction causes lock contention and slows down primary operations.

**Why it happens:** Wanting transactional consistency between business operation and its log entry.

**Consequences:**
- Business operations slower due to logging overhead
- Lock wait timeouts during high activity
- Logging failure can rollback business operation
- Database connections held longer

**Warning signs:**
- Update operations slower after adding logging
- Lock timeout errors increase
- "Add activity log" appearing in slow query logs
- Transaction timeouts during bulk operations

**Prevention:**
```typescript
// Decouple logging from business transaction
async function updateProject(id: string, data: UpdateProjectInput, userId: string) {
  // Get current state for change detection
  const before = await prisma.project.findUnique({ where: { id } });

  // Execute business operation
  const after = await prisma.project.update({
    where: { id },
    data
  });

  // Log AFTER transaction completes (fire-and-forget)
  logActivity({
    entityType: 'project',
    entityId: id,
    action: 'updated',
    changes: detectChanges(before, after),
    userId
  }).catch(err => {
    // Log failure should not affect business operation
    console.error('Failed to log activity:', err);
  });

  return after;
}

// Or use event-driven approach
const activityQueue: ActivityEntry[] = [];

async function logActivity(entry: ActivityEntry) {
  activityQueue.push(entry);

  // Batch insert every 5 seconds or 100 entries
  if (activityQueue.length >= 100) {
    await flushActivityQueue();
  }
}

// Scheduled flush
setInterval(flushActivityQueue, 5000);

async function flushActivityQueue() {
  if (activityQueue.length === 0) return;

  const batch = activityQueue.splice(0, 100);
  try {
    await prisma.activity.createMany({ data: batch });
  } catch (err) {
    console.error('Failed to flush activity queue:', err);
    // Re-queue for retry (with limit)
  }
}
```

**Phase to address:** Phase 2 (Activity Implementation) - Log asynchronously outside transaction

---

## Bidirectional Sync Pitfalls

Common mistakes when syncing data between deals/potentials and their converted projects.

### Pitfall 13: Infinite Sync Loop

**What goes wrong:** Project update triggers deal update, which triggers project update, creating infinite loop.

**Why it happens:** Both entities watch for changes in the other and trigger updates.

**Consequences:**
- Database locked in update loop
- CPU spike until timeout
- Inconsistent data after partial failure
- Application unresponsive

**Warning signs:**
- CPU spikes when editing synced entities
- Update counts in logs showing 100s of updates
- Request timeouts on simple edits
- "Maximum recursion" or "too many updates" errors

**Prevention:**
```typescript
// Use sync tokens to prevent loops
async function updateProject(id: string, data: UpdateProjectInput, options?: {
  skipSync?: boolean;
  syncToken?: string;
}) {
  const project = await prisma.project.update({
    where: { id },
    data,
    include: { sourceDeal: true, sourcePotential: true }
  });

  // Skip sync if this update came from a sync operation
  if (options?.skipSync) return project;

  // Generate unique sync token for this operation
  const syncToken = options?.syncToken || generateSyncToken();

  // Sync to source (if exists) with token to prevent loop
  if (project.sourceDeal && shouldSyncToSource(data)) {
    await syncProjectToDeal(project, project.sourceDeal.id, { syncToken });
  }

  return project;
}

async function syncProjectToDeal(project: Project, dealId: string, options: { syncToken: string }) {
  // Check if we've already processed this sync operation
  if (await isDuplicateSync(dealId, options.syncToken)) {
    console.log('Skipping duplicate sync operation');
    return;
  }

  // Mark this sync as processed
  await markSyncProcessed(dealId, options.syncToken);

  // Update deal with project data (won't trigger reverse sync)
  await updateDeal(dealId, {
    title: project.title,
    // ... synced fields
  }, { skipSync: true });
}

// Simple in-memory deduplication (for small scale)
const recentSyncs = new Map<string, number>();

function generateSyncToken(): string {
  return `sync-${Date.now()}-${Math.random().toString(36).slice(2)}`;
}

async function isDuplicateSync(entityId: string, token: string): Promise<boolean> {
  const key = `${entityId}:${token}`;
  if (recentSyncs.has(key)) return true;

  recentSyncs.set(key, Date.now());

  // Cleanup old tokens
  const cutoff = Date.now() - 60000; // 1 minute
  for (const [k, v] of recentSyncs) {
    if (v < cutoff) recentSyncs.delete(k);
  }

  return false;
}
```

**Phase to address:** Phase 1 (Sync Design) - Implement sync tokens from start

**Sources:**
- [Preventing Bidirectional Sync Loops](https://www.workato.com/product-hub/how-to-prevent-infinite-loops-in-bi-directional-data-syncs/)
- [MySQL Two-Way Sync Best Practices](https://www.bladepipe.com/docs/bestPractice/mysql_loop_data_sync/)

---

### Pitfall 14: Sync Conflicts Without Resolution Strategy

**What goes wrong:** User edits deal title while another user edits linked project title. No clear winner.

**Why it happens:** Bidirectional sync assumes single source of truth, but both sides can be edited.

**Consequences:**
- Data overwrites without warning
- "My changes disappeared" user complaints
- Inconsistent data between views
- Lost work

**Warning signs:**
- Users reporting unexpected value changes
- Same field flip-flopping between values
- "I just saved that" frustration
- Audit log showing rapid value oscillation

**Prevention:**
```typescript
// Define ownership per field
const SYNC_RULES = {
  title: { owner: 'project', syncTo: 'deal' },
  description: { owner: 'project', syncTo: 'deal' },
  value: { owner: 'deal', syncTo: 'project' }, // Only deal sets value
  status: { owner: 'project', syncTo: null }, // Not synced
  stage: { owner: 'deal', syncTo: null }, // Not synced
};

function shouldSync(field: string, direction: 'toProject' | 'toDeal'): boolean {
  const rule = SYNC_RULES[field];
  if (!rule) return false;

  if (direction === 'toProject') {
    return rule.owner === 'deal' && rule.syncTo === 'project';
  } else {
    return rule.owner === 'project' && rule.syncTo === 'deal';
  }
}

// Alternative: Timestamp-based resolution
async function resolveSyncConflict(
  local: { value: string; updatedAt: Date },
  remote: { value: string; updatedAt: Date }
): Promise<{ value: string; source: 'local' | 'remote' }> {
  // Most recent wins
  if (local.updatedAt > remote.updatedAt) {
    return { value: local.value, source: 'local' };
  }
  return { value: remote.value, source: 'remote' };
}

// Show sync status to users
function SyncIndicator({ entity, linkedEntity }: { entity: Entity; linkedEntity: Entity }) {
  const synced = entity.title === linkedEntity.title &&
                 entity.updatedAt <= linkedEntity.syncedAt;

  return (
    <Badge variant={synced ? 'success' : 'warning'}>
      {synced ? 'Synced' : 'Pending sync'}
    </Badge>
  );
}
```

**Phase to address:** Phase 1 (Sync Design) - Define ownership rules before implementing sync

---

### Pitfall 15: Sync on Every Field Change

**What goes wrong:** Any change to project triggers full sync to deal, even for fields that don't need syncing.

**Why it happens:** Simplest implementation - "project changed, sync everything."

**Consequences:**
- Unnecessary database writes
- Activity log noise from sync updates
- Potential for overwriting deal-specific data
- Performance overhead on every edit

**Warning signs:**
- Deal updatedAt changes when only project status changed
- Activity log full of "synced from project" entries
- Deal fields unexpectedly reset
- Slow saves due to sync cascade

**Prevention:**
```typescript
// Sync only changed fields that should be synced
const SYNCABLE_FIELDS = ['title', 'description'] as const;

type SyncableField = typeof SYNCABLE_FIELDS[number];

function detectSyncableChanges(
  before: Record<string, unknown>,
  after: Record<string, unknown>
): Partial<Record<SyncableField, unknown>> {
  const changes: Partial<Record<SyncableField, unknown>> = {};

  for (const field of SYNCABLE_FIELDS) {
    if (before[field] !== after[field]) {
      changes[field] = after[field];
    }
  }

  return changes;
}

async function handleProjectUpdate(
  projectId: string,
  before: Project,
  after: Project
) {
  const syncChanges = detectSyncableChanges(before, after);

  // Only sync if there are syncable changes
  if (Object.keys(syncChanges).length === 0) {
    return; // No sync needed
  }

  // Find linked deal (if any)
  const deal = await prisma.deal.findFirst({
    where: { projectId }
  });

  if (deal) {
    await prisma.deal.update({
      where: { id: deal.id },
      data: syncChanges
    });

    // Log only the synced fields
    await logActivity({
      entityType: 'deal',
      entityId: deal.id,
      action: 'synced_from_project',
      changes: syncChanges,
      userId: 'system'
    });
  }
}
```

**Phase to address:** Phase 3 (Sync Implementation) - Sync only changed syncable fields

**Sources:**
- [Bidirectional Data Sync Patterns](https://dev3lop.com/bidirectional-data-synchronization-patterns-between-systems/)
- [Data Integration Patterns](https://medium.com/@prayagvakharia/the-architects-guide-to-data-integration-patterns-migration-broadcast-bi-directional-a4c92b5f908d)

---

## Price History Tracking Pitfalls

Common mistakes when implementing supplier price history and comparison.

### Pitfall 16: Storing Price on Item Instead of History Table

**What goes wrong:** Price stored directly on SupplierItem, overwriting previous value when price changes.

**Why it happens:** Simplest model - item has a price field. Works until you need history.

**Consequences:**
- No historical price data
- Cannot compare "how much did we pay last time?"
- Cannot track price trends
- Lost business intelligence

**Warning signs:**
- "What was the old price?" questions unanswerable
- No visibility into price changes over time
- Cannot validate supplier price increases
- Reporting shows only current prices

**Prevention:**
```prisma
// Store current price AND maintain history
model SupplierItem {
  id           String   @id @default(cuid())
  supplierId   String   @map("supplier_id")
  supplier     Supplier @relation(fields: [supplierId], references: [id])
  description  String
  unit         String   // 'each', 'box', 'kg'
  currentPrice Decimal? @map("current_price") @db.Decimal(12, 2)

  priceHistory PriceHistory[]

  @@unique([supplierId, description]) // Prevent duplicate items per supplier
  @@map("supplier_items")
}

model PriceHistory {
  id            String       @id @default(cuid())
  itemId        String       @map("item_id")
  item          SupplierItem @relation(fields: [itemId], references: [id], onDelete: Cascade)
  price         Decimal      @db.Decimal(12, 2)
  effectiveDate DateTime     @map("effective_date")
  sourceDoc     String?      @map("source_doc") // Quote/invoice reference
  notes         String?

  createdAt     DateTime     @default(now())

  @@index([itemId, effectiveDate])
  @@map("price_history")
}
```

```typescript
// Update price with history tracking
async function updateSupplierItemPrice(
  itemId: string,
  newPrice: number,
  effectiveDate: Date,
  sourceDoc?: string
) {
  // Create history entry AND update current price atomically
  return prisma.$transaction([
    prisma.priceHistory.create({
      data: {
        itemId,
        price: newPrice,
        effectiveDate,
        sourceDoc
      }
    }),
    prisma.supplierItem.update({
      where: { id: itemId },
      data: { currentPrice: newPrice }
    })
  ]);
}
```

**Phase to address:** Phase 1 (Supplier Schema) - Design price history table from start

**Sources:**
- [Designing A Price History Database Model](https://vertabelo.com/blog/price-history-database-model/)
- [Redgate Price History Design](https://www.red-gate.com/blog/price-history-database-model)

---

### Pitfall 17: No Effective Date on Price Records

**What goes wrong:** Price history entries have createdAt (when inserted) but not effectiveDate (when price became active).

**Why it happens:** Confusion between "when we learned about the price" vs "when the price was valid from."

**Consequences:**
- Cannot determine price at a specific date
- Quote dated Jan 1 but entered Jan 15 shows wrong in Jan 5 comparisons
- Historical cost calculations incorrect
- Retroactive quotes break timeline

**Warning signs:**
- "Price as of date X" queries return wrong results
- Backdated quotes appear in wrong order
- Historical reports don't match actual costs
- Users confused about which price applies

**Prevention:**
```typescript
// Always require effective date
interface CreatePriceInput {
  itemId: string;
  price: number;
  effectiveDate: Date; // Required!
  sourceDoc?: string;
}

// Query price at a specific date
async function getPriceAtDate(itemId: string, asOfDate: Date): Promise<number | null> {
  const priceRecord = await prisma.priceHistory.findFirst({
    where: {
      itemId,
      effectiveDate: { lte: asOfDate }
    },
    orderBy: { effectiveDate: 'desc' }
  });

  return priceRecord?.price ? Number(priceRecord.price) : null;
}

// Get price trend over time
async function getPriceTrend(itemId: string, startDate: Date, endDate: Date) {
  return prisma.priceHistory.findMany({
    where: {
      itemId,
      effectiveDate: {
        gte: startDate,
        lte: endDate
      }
    },
    orderBy: { effectiveDate: 'asc' }
  });
}
```

**Phase to address:** Phase 1 (Supplier Schema) - Make effectiveDate required

---

### Pitfall 18: Price Stored Without Unit Context

**What goes wrong:** Price of "50.00" stored without knowing if it's per unit, per box, per kg, etc.

**Why it happens:** Assumption that unit is obvious or consistent across suppliers.

**Consequences:**
- Comparing apples to oranges (literally)
- "Supplier A is cheaper" when they sell by box, B sells by unit
- Wrong cost calculations
- Misleading price comparisons

**Warning signs:**
- Same product shows wildly different prices across suppliers
- "That can't be right" reactions to comparisons
- Manual normalization needed for reports
- Users adding units to description field

**Prevention:**
```prisma
model SupplierItem {
  id           String   @id @default(cuid())
  description  String
  unit         String   // 'each', 'box', 'pack', 'kg', 'liter'
  unitsPerPack Int?     @map("units_per_pack") // If unit is 'box', how many per box?
  currentPrice Decimal? @map("current_price") @db.Decimal(12, 2)

  // ...
}
```

```typescript
// Normalize prices for comparison
function normalizeToUnitPrice(item: SupplierItem): number {
  const price = Number(item.currentPrice);

  switch (item.unit) {
    case 'each':
      return price;
    case 'box':
    case 'pack':
      return item.unitsPerPack ? price / item.unitsPerPack : price;
    case 'dozen':
      return price / 12;
    default:
      return price; // Can't normalize, show as-is
  }
}

// Display with context
function PriceDisplay({ item }: { item: SupplierItem }) {
  const unitPrice = normalizeToUnitPrice(item);

  return (
    <div>
      <span className="font-bold">RM {item.currentPrice}</span>
      <span className="text-muted-foreground">
        {' '}/ {item.unit}
        {item.unit !== 'each' && (
          <span className="ml-2">(RM {unitPrice.toFixed(2)} each)</span>
        )}
      </span>
    </div>
  );
}
```

**Phase to address:** Phase 1 (Supplier Schema) - Include unit and pack size in item model

---

## Integration Complexity Pitfalls

Common mistakes when adding many relationships between entities.

### Pitfall 19: Over-Fetching Due to Deep Includes

**What goes wrong:** Every query includes all related entities "just in case," leading to massive query results.

**Why it happens:** Convenience of having all data available. Fear of making additional queries.

**Consequences:**
- Slow API responses
- Large JSON payloads over network
- Memory pressure on server
- Client-side processing overhead

**Warning signs:**
- API responses contain 10x more data than displayed
- Response time grows with relationship count
- "Include everything" patterns in code
- Large network payloads in dev tools

**Prevention:**
```typescript
// Define explicit field sets for different contexts
const SUPPLIER_LIST_SELECT = {
  id: true,
  name: true,
  itemCount: true,
  lastOrderDate: true
} as const;

const SUPPLIER_DETAIL_SELECT = {
  ...SUPPLIER_LIST_SELECT,
  address: true,
  contactName: true,
  contactEmail: true,
  contactPhone: true,
  creditTerms: true,
  paymentTerms: true,
  notes: true
} as const;

const SUPPLIER_WITH_ITEMS_SELECT = {
  ...SUPPLIER_DETAIL_SELECT,
  items: {
    select: {
      id: true,
      description: true,
      currentPrice: true,
      unit: true
    },
    take: 50 // Limit items returned
  }
} as const;

// Use appropriate select for context
async function getSuppliers() {
  return prisma.supplier.findMany({
    select: SUPPLIER_LIST_SELECT // Only what list view needs
  });
}

async function getSupplierDetail(id: string) {
  return prisma.supplier.findUnique({
    where: { id },
    select: SUPPLIER_DETAIL_SELECT // Detail view needs more
  });
}
```

**Phase to address:** All phases - Define select sets per use case from start

---

### Pitfall 20: Circular Reference Serialization Errors

**What goes wrong:** Entities with bidirectional relationships cause JSON serialization to fail with circular reference errors.

**Why it happens:** Prisma returns objects with actual references, not plain data. Serializing these hits circular dependencies.

**Consequences:**
- "Converting circular structure to JSON" errors
- API endpoints failing
- Client receiving corrupted data
- Server crashes on response serialization

**Warning signs:**
- "TypeError: Converting circular structure to JSON"
- API returns 500 errors when including related entities
- Works without includes, fails with them
- Infinite loops in logging

**Prevention:**
```typescript
// Option 1: Explicit select (preferred)
const deal = await prisma.deal.findUnique({
  where: { id },
  select: {
    id: true,
    title: true,
    project: {
      select: {
        id: true,
        title: true
        // Don't select project.sourceDeal (circular)
      }
    }
  }
});

// Option 2: Transform to DTO before sending
function dealToDTO(deal: DealWithProject): DealDTO {
  return {
    id: deal.id,
    title: deal.title,
    project: deal.project ? {
      id: deal.project.id,
      title: deal.project.title
    } : null
  };
}

// Option 3: Use superjson for complex cases
import superjson from 'superjson';

export async function GET(request: Request) {
  const deal = await prisma.deal.findUnique({
    where: { id },
    include: { project: true }
  });

  // superjson handles circular references
  return new Response(superjson.stringify(deal), {
    headers: { 'Content-Type': 'application/json' }
  });
}
```

**Phase to address:** All phases - Use select or DTOs for API responses

---

### Pitfall 21: Missing Indexes on Foreign Keys

**What goes wrong:** Queries filtering by relationships become slow as data grows because foreign keys aren't indexed.

**Why it happens:** Prisma creates foreign key constraints but doesn't always auto-index them. Developers assume indexes exist.

**Consequences:**
- Slow queries on related entity lookups
- Full table scans on JOIN operations
- Performance degradation with data growth
- Database CPU spikes on relationship queries

**Warning signs:**
- "Find costs for project" queries slow down
- EXPLAIN shows "Using filesort" or "Using temporary"
- Query time grows linearly with table size
- MariaDB slow query log filling up

**Prevention:**
```prisma
// Explicitly add indexes on all foreign keys
model Cost {
  id          String       @id @default(cuid())
  projectId   String       @map("project_id")
  project     Project      @relation(fields: [projectId], references: [id], onDelete: Cascade)
  supplierId  String?      @map("supplier_id")
  supplier    Supplier?    @relation(fields: [supplierId], references: [id])
  categoryId  String       @map("category_id")
  category    CostCategory @relation(fields: [categoryId], references: [id])
  // ...

  @@index([projectId])      // Costs by project
  @@index([supplierId])     // Costs by supplier
  @@index([categoryId])     // Costs by category
  @@index([projectId, categoryId]) // Project costs by category
  @@map("costs")
}

model SupplierItem {
  id          String   @id @default(cuid())
  supplierId  String   @map("supplier_id")
  supplier    Supplier @relation(fields: [supplierId], references: [id])
  // ...

  @@index([supplierId])    // Items by supplier
  @@index([supplierId, description]) // Lookup by supplier + description
  @@map("supplier_items")
}
```

**Phase to address:** Phase 1 (All Schemas) - Add indexes on all foreign keys and common query patterns

---

## Phase-Specific Warnings Summary

| Phase | Topic | Likely Pitfall | Mitigation |
|-------|-------|---------------|------------|
| Phase 1 | Task Schema | Unbounded recursive includes | Use CTE queries, build tree client-side |
| Phase 1 | Task Schema | Cascade delete scope | Use SetNull, implement explicit tree deletion |
| Phase 1 | Supplier Schema | Price without history | Create PriceHistory table from start |
| Phase 1 | Supplier Schema | Missing unit context | Include unit and pack size fields |
| Phase 1 | Sync Design | Infinite loop potential | Implement sync tokens/markers |
| Phase 1 | Sync Design | No conflict resolution | Define field ownership rules |
| Phase 2 | Task CRUD | No depth enforcement | Limit nesting to 5 levels |
| Phase 2 | Task CRUD | Cycle creation on move | Validate ancestry before reparenting |
| Phase 2 | Activity Log | Single bloated table | Per-entity tables or polymorphic with indexes |
| Phase 2 | Activity Log | No retention policy | Implement tiered retention from start |
| Phase 3 | Task UI | Full tree rendering | Virtualize or lazy-load children |
| Phase 3 | Sync Implementation | Sync on every change | Sync only changed syncable fields |
| Phase 4 | AI Matching | Numeric comparison via embedding | Hybrid semantic + structured filtering |
| Phase 4 | AI Matching | Single threshold truth | Confidence levels with human review |
| Phase 4 | AI Matching | No fallback for API failure | Cache embeddings, implement keyword fallback |
| Phase 4 | AI Costs | Re-embedding same text | Batch embed on import, cache results |
| All | Relationships | Over-fetching | Define select sets per context |
| All | Relationships | Circular JSON errors | Use explicit select or DTOs |
| All | Performance | Missing FK indexes | Add indexes on all foreign keys |

---

## Security Checklist for v1.4

Before deploying task hierarchies:
- [ ] Maximum depth enforced server-side
- [ ] Cycle prevention on task moves
- [ ] Cascade delete requires confirmation for large trees
- [ ] Tree queries have LIMIT clauses

Before deploying AI matching:
- [ ] Embedding API key secured in environment
- [ ] Fallback to keyword search on API failure
- [ ] Similarity scores shown to users (transparency)
- [ ] Human review for LOW confidence matches

Before deploying activity logging:
- [ ] Sensitive fields excluded from logs
- [ ] Retention policy documented
- [ ] Logs not included in standard exports
- [ ] User can see their own activity only (unless admin)

Before deploying bidirectional sync:
- [ ] Sync tokens prevent infinite loops
- [ ] Field ownership clearly documented
- [ ] Sync failures don't block primary operations
- [ ] Activity log shows sync source

Before deploying supplier management:
- [ ] Price changes create history (not overwrite)
- [ ] Unit context required on all prices
- [ ] Supplier deletion handles orphaned items
- [ ] Price comparison shows unit normalization

---

## Pre-Implementation Checklist

- [ ] Task model uses adjacency list (parentId self-relation)
- [ ] PriceHistory table designed with effectiveDate
- [ ] Activity table strategy chosen (per-entity or polymorphic)
- [ ] Sync rules documented (which fields, which direction)
- [ ] Embedding provider selected (Voyage AI recommended)
- [ ] Embedding cache column on SupplierItem
- [ ] Indexes on all foreign keys
- [ ] Maximum task depth constant defined (5 recommended)
- [ ] Retention policy constants defined
- [ ] Sync token generation utility ready

---

## Sources

### Hierarchical Data / Tree Structures
- [Prisma Self-Relations Documentation](https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/self-relations)
- [Prisma Issue #3725 - Recursive Relationships](https://github.com/prisma/prisma/issues/3725)
- [NestJS Recursive Relationships with Prisma](https://wanago.io/2023/12/11/api-nestjs-sql-recursive-relationships-prisma-postgresql/)
- [MySQL Adjacency List with CTE](https://akki.ca/blog/mysql-adjacency-list-model-for-hierarchical-data-using-cte/)
- [Adjacency List vs Nested Sets](https://explainextended.com/2009/09/29/adjacency-list-vs-nested-sets-mysql/)

### AI/Embeddings
- [Claude Embeddings Documentation](https://docs.claude.com/en/docs/build-with-claude/embeddings)
- [Semantic Similarity Guide 2025](https://www.shadecoder.com/topics/semantic-similarity-a-comprehensive-guide-for-2025)
- [OpenAI Community - Embeddings Price Problem](https://community.openai.com/t/ecommerce-product-catalog-search-embeddings-problem-with-price/1036804)
- [Embedding Models in 2025](https://medium.com/@alex-azimbaev/embedding-models-in-2025-technology-pricing-practical-advice-2ed273fead7f)
- [Best Embedding Models for RAG](https://greennode.ai/blog/best-embedding-models-for-rag)

### Activity Logging
- [Database Design for Audit Logging](https://vertabelo.com/blog/database-design-for-audit-logging/)
- [Martin Fowler - Audit Log Pattern](https://martinfowler.com/eaaDev/AuditLog.html)
- [Redgate Audit Logging Design](https://www.red-gate.com/blog/database-design-for-audit-logging)

### Bidirectional Sync
- [Preventing Bidirectional Sync Loops](https://www.workato.com/product-hub/how-to-prevent-infinite-loops-in-bi-directional-data-syncs/)
- [MySQL Two-Way Sync Best Practices](https://www.bladepipe.com/docs/bestPractice/mysql_loop_data_sync/)
- [Bidirectional Data Sync Patterns](https://dev3lop.com/bidirectional-data-synchronization-patterns-between-systems/)

### Price History
- [Designing A Price History Database Model](https://vertabelo.com/blog/price-history-database-model/)
- [Redgate Price History Design](https://www.red-gate.com/blog/price-history-database-model)

### Supplier/ERD Design
- [ER Diagrams for Supply Chain](https://www.geeksforgeeks.org/sql/how-to-design-er-diagrams-for-supply-chain-management/)
- [ERD Relationships Guide](https://creately.com/guides/erd-relationships/)

---

*Pitfalls research: 2026-01-24 for SAAP v1.4*
